{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Year Project - Times of Malta Scraper Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this section, a dataset is created by scraping for article images and their corresponding information from the Times of Malta website. This code iterates through each article found when searching with the keyword 'inset' and views each of them one by one. Whenever it clicks into an article, the following information is retrieved:\n",
    "\n",
    "- URL\n",
    "- Article Title\n",
    "- Date Published\n",
    "- Categories\n",
    "- Images\n",
    "- Image Captions\n",
    "\n",
    "This information is stored within two CSV files, these being:\n",
    "\n",
    "- Article_Information.csv - This CSV file stores the URL, Article Title, Date Published, Categories and the names of the images in the article.\n",
    "- Image_Information.csv - This CSV file stores the URL, Image Name, Image Caption as well as an attribute called 'Inset'. This attribute is responsible for storing whether the corresponding image is an inset image or not.\n",
    "\n",
    "All information is retrieved by accessing the respective HTML elements, the only exception to this being the 'Inset' attribute, which is set to True whenever the word 'inset' is found in the corresponding image caption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing / Importing Packages\n",
    "\n",
    "The following packages are required for the notebook to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing and Importing Packages\n",
    "import os\n",
    "import requests\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from datetime import datetime\n",
    "from datetime import date as dateToday\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating WebScraper Class\n",
    "\n",
    "This class is used to define paths which will be used throughout the rest of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining class called WebScraper which has a path to the ChromeDriver executable file, \n",
    "#the folder in which images will be saved and the path where the CSV files will be saved.\n",
    "class WebScraper:\n",
    "    def __init__(self, folderName = 'data'):\n",
    "        self.CHROME_DRIVER_PATH = \".\\chromedriver_win32\\chromedriver.exe\"                                      \n",
    "        self.NEWS_IMG_PATH = os.path.join(folderName, 'img')\n",
    "        self.NEWS_PATH = folderName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Function to get Image Extension\n",
    "\n",
    "This function checks the image extension type are returns the respective type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImageExtension(imageLink):\n",
    "    if imageLink.endswith('.jpeg'):\n",
    "        return '.jpeg'\n",
    "    elif imageLink.endswith('.jpg'):\n",
    "        return '.jpg'\n",
    "    elif imageLink.endswith('.png'):\n",
    "        return '.png'\n",
    "    else: return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Function to Scrape Website and Save Information in CSV Files\n",
    "\n",
    "In this section, an instance of the WebScraper class defined earlier is created and the appropriate settings are made to connect to the Times of Malta website. The information specified above is retrieved and saved in the csv files whilst also saving all images in the img folder. The appropriate error checking and exception handling is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapingFunction(folderName, website, numberOfArticles):\n",
    "\n",
    "    #Create an instance of the WebScraper Class\n",
    "    webScraper = WebScraper(folderName = folderName)\n",
    "\n",
    "    #Getting Chrome options and disabling Chrome Logging Messages\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "    #Create an instance of a Service Object\n",
    "    service = Service(executable_path = webScraper.CHROME_DRIVER_PATH)\n",
    "\n",
    "    #Create an instance of a driver used to control Chrome\n",
    "    driver = webdriver.Chrome(service = service, options = options)\n",
    "\n",
    "    #Opening the website from which content will be scraped\n",
    "    driver.get(website)\n",
    "\n",
    "    #Create lists of data to be stored\n",
    "    articleData = []\n",
    "    imageData = []\n",
    "\n",
    "    #Create counters\n",
    "    count = 0\n",
    "    articleIndex = 0\n",
    "    pageIndex = 0\n",
    "\n",
    "    #Check for Cookies Consent button, if it is displayed, click the Consent Button\n",
    "    try: \n",
    "        WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//div[@class='fc-consent-root']//button[@aria-label='Consent']//p[@class='fc-button-label' and text()='Consent']\"))).click()\n",
    "    except: \n",
    "        pass\n",
    " \n",
    "    #Loop for n amount of times, where n is the number of articles to be scraped\n",
    "    for i in range(numberOfArticles):\n",
    "        try: \n",
    "            myobj = datetime.now()\n",
    "            today = dateToday.today()\n",
    "            #Click on the Article\n",
    "            driver.find_element(By.XPATH,f'//*[@id=\"listing-articles\"]/div[{str(2+articleIndex)}]/a').click()\n",
    "            #Increment the Article Index\n",
    "            articleIndex += 1\n",
    "\n",
    "            #Wait for the contents to load\n",
    "            sleep(1)\n",
    "            #Get the current URL\n",
    "            url = driver.current_url\n",
    "            #Get the Article Title\n",
    "            title = driver.find_element(By.XPATH,'//*[@id=\"article-head\"]/div/h1').text\n",
    "            #Get the Article Date of Publication\n",
    "            date = driver.find_element(By.CLASS_NAME,'wi-WidgetMeta-time').text\n",
    "\n",
    "            if date.endswith(\"ago\"):\n",
    "                if int(myobj.hour) - int(date[0:2]) >= 0:\n",
    "                    date = today.strftime(\"%d-%b-%Y\")\n",
    "                else:\n",
    "                    date = (today - timedelta(days = 1)).strftime(\"%d-%b-%Y\")\n",
    "\n",
    "            #Get the Category Names and remove any new lines, instead add commas as delimeters\n",
    "            categoryNames = driver.find_elements(By.XPATH, '//*[@id=\"article-head\"]/div/div')[0].text\n",
    "            categories = \"\"\n",
    "            for category in categoryNames:\n",
    "                if category != '\\n':\n",
    "                    categories += category\n",
    "                else:\n",
    "                    categories += ','\n",
    "            \n",
    "            #Get Article Thumbnail and Images\n",
    "            imageLinks = [image for image in driver.find_elements(By.XPATH,'//*[@id=\"observer\"]/main/article/div[2]/div/*/img') + driver.find_elements(By.XPATH,'//*[@id=\"article-head\"]/div/picture/img')]\n",
    "            \n",
    "            #Create images and author variables as empty strings\n",
    "            images = \"\"\n",
    "            author = \"\"\n",
    "\n",
    "            #Get author name\n",
    "            try:\n",
    "                author = driver.find_element(By.CLASS_NAME,'wi-WidgetMeta-author').text[2:]\n",
    "            except:\n",
    "                author = \"N/A\"\n",
    "                pass\n",
    "\n",
    "            #Write images to disk\n",
    "            for imageLink in imageLinks:\n",
    "                #Get Image Source\n",
    "                imageSource = imageLink.get_attribute('src')\n",
    "                #Get Image Caption\n",
    "                caption = imageLink.get_attribute('alt')\n",
    "                #Get Image Extension\n",
    "                imageExtension  = getImageExtension(imageSource)      \n",
    "                #Create Image Name by appending the extension to the count value\n",
    "                imageName = f'img{str(count).zfill(5)}' + imageExtension\n",
    "                #Append Image Name to list\n",
    "                images  += imageName + ','           \n",
    "                #Download Image          \n",
    "                img_data = requests.get(imageSource).content       \n",
    "                #Check if 'inset' or 'Inset' is in caption, if it is set the attribute to True, else keep it False\n",
    "                insetBool = False\n",
    "                if \"inset\" in caption or \"Inset\" in caption:\n",
    "                    insetBool = True\n",
    "                #Append a list containing the URL, Image Name, Caption and Inset Boolean variable to the imageData list\n",
    "                imageData.append([url, imageName, caption, insetBool])\n",
    "                #Creating a dataframe using imageData and saving it in a CSV file named Image_Information.csv#\n",
    "                pd.DataFrame(columns=['URL', 'Image Name', 'Caption', 'Inset'], data=imageData).to_csv(os.path.join(webScraper.NEWS_PATH,'Image_Information.csv'), index=False)\n",
    "                #Incrementing the row counter\n",
    "                count += 1\n",
    "\n",
    "                #Save the images\n",
    "                with open(os.path.join(webScraper.NEWS_IMG_PATH, imageName),'wb') as file:\n",
    "                    file.write(img_data)\n",
    "\n",
    "            #Append a list containing the URL, Article Title, Article Date of Publication, Categories and Image Names to the articleData list\n",
    "            articleData.append([url, title, author, date, categories, images[:-1]])\n",
    "            #Creating a dataframe using articleData and saving it in a CSV file named Article_Information.csv\n",
    "            pd.DataFrame(columns=['URL', 'Article Name', 'Author', 'Date', 'Categories', 'Images'], data=articleData).to_csv(os.path.join(webScraper.NEWS_PATH,'Article_Information.csv'), index=False)\n",
    "\n",
    "            #Go back to previous page\n",
    "            driver.back()\n",
    "\n",
    "        #Exception where element is not found\n",
    "        except selenium.common.exceptions.NoSuchElementException as e:\n",
    "            #Wait for the contents to load\n",
    "            sleep(1.5)\n",
    "\n",
    "            try: \n",
    "                #Go to the Next Page\n",
    "                driver.find_element(By.XPATH,f'//*[@id=\"observer\"]/main/div/div[2]/div/span[{str(2+pageIndex)}]').click()\n",
    "            except: \n",
    "                #Display Error Message and go back to Google Home Page\n",
    "                print(f\"ERROR: Reloading page {pageIndex+2}\")\n",
    "                driver.get(f\"https://google.com\")\n",
    "\n",
    "                #Wait for the contents to load\n",
    "                sleep(2)\n",
    "\n",
    "                #Increment Page Index\n",
    "                pageIndex += 1\n",
    "                #Load the website again, skipping the previous page\n",
    "                driver.get(website.split(\"page\")[0] + f'page={str(pageIndex+2)}')\n",
    "                #Reset articleIndex\n",
    "                articleIndex = 0\n",
    "                \n",
    "                continue\n",
    "\n",
    "            #Increment pageIndex\n",
    "            pageIndex += 1\n",
    "            #Reset articleIndex\n",
    "            articleIndex = 0\n",
    "        \n",
    "        #Exception where element could not be clicked\n",
    "        except selenium.common.exceptions.ElementClickInterceptedException as e:\n",
    "            #Display Error Message\n",
    "            print('ERROR: Click Intercepted - Skipping')\n",
    "            #Increment articleIndex\n",
    "            articleIndex += 1\n",
    "\n",
    "            continue\n",
    "        \n",
    "        #Exception for other cases\n",
    "        except Exception as e:\n",
    "            #Display Error Message\n",
    "            print(str(e))\n",
    "\n",
    "            #Increment articleIndex\n",
    "            articleIndex += 1\n",
    "            #Increment pageIndex\n",
    "            pageIndex += 1\n",
    "            \n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling Scraping Functions\n",
    "\n",
    "In this section, we call scrapingFunction() and pass the folder in which images will be saved, the website to scraper from and the number of articles to scrape. In this case, articles are being scraped from the list of articles given when querying the 'inset' keyword in the Times of Malta website as well as their National, World, Opinion, Sport and Business sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Click Intercepted - Skipping\n",
      "ERROR: Reloading page 3\n",
      "ERROR: Click Intercepted - Skipping\n",
      "ERROR: Reloading page 12\n",
      "ERROR: Click Intercepted - Skipping\n",
      "ERROR: Reloading page 8\n",
      "ERROR: Reloading page 10\n",
      "ERROR: Reloading page 11\n",
      "ERROR: Reloading page 12\n",
      "ERROR: Reloading page 13\n",
      "ERROR: Click Intercepted - Skipping\n",
      "ERROR: Reloading page 12\n",
      "ERROR: Click Intercepted - Skipping\n",
      "ERROR: Reloading page 12\n",
      "ERROR: Reloading page 3\n",
      "ERROR: Click Intercepted - Skipping\n",
      "Message: unknown error: cannot determine loading status\n",
      "from no such window\n",
      "  (Session info: chrome=115.0.5790.171)\n",
      "Stacktrace:\n",
      "Backtrace:\n",
      "\tGetHandleVerifier [0x010FA813+48355]\n",
      "\t(No symbol) [0x0108C4B1]\n",
      "\t(No symbol) [0x00F95220]\n",
      "\t(No symbol) [0x00F888E2]\n",
      "\t(No symbol) [0x00F87138]\n",
      "\t(No symbol) [0x00F877AA]\n",
      "\t(No symbol) [0x00F908A9]\n",
      "\t(No symbol) [0x00F9C668]\n",
      "\t(No symbol) [0x00F9F566]\n",
      "\t(No symbol) [0x00F87BC3]\n",
      "\t(No symbol) [0x00F9C37A]\n",
      "\t(No symbol) [0x00FECB1F]\n",
      "\t(No symbol) [0x00FDA536]\n",
      "\t(No symbol) [0x00FB82DC]\n",
      "\t(No symbol) [0x00FB93DD]\n",
      "\tGetHandleVerifier [0x0135AABD+2539405]\n",
      "\tGetHandleVerifier [0x0139A78F+2800735]\n",
      "\tGetHandleVerifier [0x0139456C+2775612]\n",
      "\tGetHandleVerifier [0x011851E0+616112]\n",
      "\t(No symbol) [0x01095F8C]\n",
      "\t(No symbol) [0x01092328]\n",
      "\t(No symbol) [0x0109240B]\n",
      "\t(No symbol) [0x01084FF7]\n",
      "\tBaseThreadInitThunk [0x75ED7D59+25]\n",
      "\tRtlInitializeExceptionChain [0x7742B79B+107]\n",
      "\tRtlClearBits [0x7742B71F+191]\n",
      "\n",
      "ERROR: Reloading page 10\n",
      "ERROR: Reloading page 11\n",
      "ERROR: Reloading page 12\n",
      "ERROR: Reloading page 13\n",
      "ERROR: Reloading page 14\n"
     ]
    }
   ],
   "source": [
    "scrapingFunction('.\\\\TOM_Dataset\\\\TOM_Dataset_Inset', 'https://timesofmalta.com/search?keywords=inset&author=0&tags=0&sort=date&order=desc&fields%5B0%5D=title&fields%5B1%5D=body&page=1', 100)\n",
    "scrapingFunction('.\\\\TOM_Dataset\\\\TOM_Dataset_National', 'https://timesofmalta.com/articles/listing/national/page=1', 200)\n",
    "scrapingFunction('.\\\\TOM_Dataset\\\\TOM_Dataset_World', 'https://timesofmalta.com/articles/listing/world/page=1', 200)\n",
    "scrapingFunction('.\\\\TOM_Dataset\\\\TOM_Dataset_Opinion', 'https://timesofmalta.com/articles/listing/opinion/page=1', 200)\n",
    "scrapingFunction('.\\\\TOM_Dataset\\\\TOM_Dataset_Sport', 'https://timesofmalta.com/articles/listing/sport/page=1', 200)\n",
    "scrapingFunction('.\\\\TOM_Dataset\\\\TOM_Dataset_Business', 'https://timesofmalta.com/articles/listing/business/page=1', 200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
