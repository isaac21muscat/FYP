{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO:\n",
    "\n",
    "### STEP 1\n",
    "- Contents CSV 1 (Image Name, Caption, Inset Boolean)\n",
    "- Contents CSV 2 (Article URL, Article Name, Categories, Date, List of Image Names)\n",
    "- Draw ERD\n",
    "\n",
    "### STEP 2\n",
    "- Get most recent articles (example: past week or past 100 articles)\n",
    "- Perform same functionality as in step 1\n",
    "- Calculate statistics (To be discussed in further detail)\n",
    "\n",
    "### STEP 3\n",
    "- Research Image-in-Image and Picture-in-Picture Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Year Project - Emoji Inset Creator Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this section, a dataset is created by scraping for article images and their corresponding information from the Times of Malta website. This code iterates through each article found when searching with the keyword 'inset' and views each of them one by one. Whenever it clicks into an article, the following information is retrieved:\n",
    "\n",
    "- URL\n",
    "- Article Title\n",
    "- Date Published\n",
    "- Categories\n",
    "- Images\n",
    "- Image Captions\n",
    "\n",
    "This information is stored within two CSV files, these being:\n",
    "\n",
    "- Article_Information.csv - This CSV file stores the URL, Article Title, Date Published, Categories and the names of the images in the article.\n",
    "- Image_Information.csv - This CSV file stores the URL, Image Name, Image Caption as well as an attribute called 'Inset'. This attribute is responsible for storing whether the corresponding image is an inset image or not.\n",
    "\n",
    "All information is retrieved by accessing the respective HTML elements, the only exception to this being the 'Inset' attribute, which is set to True whenever the word 'inset' is found in the corresponding image caption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing / Importing Packages\n",
    "\n",
    "The following packages are required for the notebook to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing and Importing Packages\n",
    "import os\n",
    "import requests\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating WebScraper Class\n",
    "\n",
    "This class is used to define paths which will be used throughout the rest of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining class called WebScraper which has a path to the ChromeDriver executable file, \n",
    "#the folder in which images will be saved and the path where the CSV files will be saved.\n",
    "class WebScraper:\n",
    "    def __init__(self, folderName = 'data'):\n",
    "        self.CHROME_DRIVER_PATH = \".\\chromedriver_win32\\chromedriver.exe\"                                      \n",
    "        self.NEWS_IMG_PATH = os.path.join(folderName, 'img')\n",
    "        self.NEWS_PATH = folderName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Function to get Image Extension\n",
    "\n",
    "This function checks the image extension type are returns the respective type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImageExtension(imageLink):\n",
    "    if imageLink.endswith('.jpeg'):\n",
    "        return '.jpeg'\n",
    "    elif imageLink.endswith('.jpg'):\n",
    "        return '.jpg'\n",
    "    elif imageLink.endswith('.png'):\n",
    "        return '.png'\n",
    "    else: return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting Chrome Driver to Times of Malta Website\n",
    "\n",
    "In this section, an instance of the WebScraper class defined earlier is created and the appropriate settings are made to connect to the Times of Malta website, using the keyword 'inset' to search through articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an instance of the WebScraper Class\n",
    "webScraper = WebScraper(folderName = '.\\\\times_of_malta')\n",
    "\n",
    "#Getting Chrome options and disabling Chrome Logging Messages\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "#Create an instance of a Service Object\n",
    "service = Service(executable_path = webScraper.CHROME_DRIVER_PATH)\n",
    "\n",
    "#Create an instance of a driver used to control Chrome\n",
    "driver = webdriver.Chrome(service = service, options = options)\n",
    "\n",
    "#Opening the website from which content will be scraped\n",
    "driver.get('https://timesofmalta.com/search?keywords=inset&author=0&tags=0&sort=date&order=desc&fields%5B0%5D=title&fields%5B1%5D=body&page=1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Website and Saving Information in CSV Files\n",
    "\n",
    "In this section, the information specified above is retrieved and saved in the csv files whilst also saving all images in the img folder. The appropriate error checking and exception handling is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create lists of data to be stored\n",
    "articleData = []\n",
    "imageData = []\n",
    "\n",
    "#Create counters\n",
    "count = 0\n",
    "articleIndex = 0\n",
    "pageIndex = 0\n",
    "\n",
    "#Check for Cookies Consent button, if it is displayed, click the Consent Button\n",
    "try: \n",
    "    WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//div[@class='fc-consent-root']//button[@aria-label='Consent']//p[@class='fc-button-label' and text()='Consent']\"))).click()\n",
    "except: \n",
    "    pass\n",
    "\n",
    "#Loop for n amount of times, where n is the number of articles to be scraped\n",
    "for i in range(5):\n",
    "    try: \n",
    "\n",
    "        #Click on the Article\n",
    "        driver.find_element(By.XPATH,f'//*[@id=\"listing-articles\"]/div[{str(2+articleIndex)}]/a').click()\n",
    "        #Increment the Article Index\n",
    "        articleIndex += 1\n",
    "\n",
    "        #Wait for the contents to load\n",
    "        sleep(1)\n",
    "        #Get the current URL\n",
    "        url = driver.current_url\n",
    "        #Get the Article Title\n",
    "        title = driver.find_element(By.XPATH,'//*[@id=\"article-head\"]/div/h1').text\n",
    "        #Get the Article Date of Publication\n",
    "        date = driver.find_element(By.CLASS_NAME,'wi-WidgetMeta-time').text\n",
    "        #Get the Category Names and remove any new lines, instead add commas as delimeters\n",
    "        categoryNames = driver.find_elements(By.XPATH, '//*[@id=\"article-head\"]/div/div')[0].text\n",
    "        categories = \"\"\n",
    "        for category in categoryNames:\n",
    "            if category != '\\n':\n",
    "                categories += category\n",
    "            else:\n",
    "                categories += ','\n",
    "        \n",
    "        #Get Article Thumbnail and Images\n",
    "        imageLinks = [image for image in driver.find_elements(By.XPATH,'//*[@id=\"observer\"]/main/article/div[2]/div/*/img') + driver.find_elements(By.XPATH,'//*[@id=\"article-head\"]/div/picture/img')]\n",
    "        \n",
    "        #Create images and captions variables as empty strings\n",
    "        captions = \"\"\n",
    "        images = \"\"\n",
    "\n",
    "        #Write images to disk\n",
    "        for imageLink in imageLinks:\n",
    "            #Get Image Source\n",
    "            imageSource = imageLink.get_attribute('src')\n",
    "            #Get Image Caption\n",
    "            caption = imageLink.get_attribute('alt')\n",
    "            #Get Image Extension\n",
    "            imageExtension  = getImageExtension(imageSource)      \n",
    "            #Create Image Name by appending the extension to the count value\n",
    "            imageName = f'img{str(count).zfill(5)}' + imageExtension\n",
    "            #Append Image Name to list\n",
    "            images  += imageName + ','           \n",
    "            #Download Image          \n",
    "            img_data = requests.get(imageSource).content       \n",
    "            #Check if 'inset' or 'Inset' is in caption, if it is set the attribute to True, else keep it False\n",
    "            insetBool = False\n",
    "            if \"inset\" in caption or \"Inset\" in caption:\n",
    "                insetBool = True\n",
    "            #Append a list containing the URL, Image Name, Caption and Inset Boolean variable to the imageData list\n",
    "            imageData.append([url, imageName, caption, insetBool])\n",
    "            #Creating a dataframe using imageData and saving it in a CSV file named Image_Information.csv#\n",
    "            pd.DataFrame(columns=['URL', 'Image Name', 'Caption', 'Inset'], data=imageData).to_csv(os.path.join(webScraper.NEWS_PATH,'Image_Information.csv'), index=False)\n",
    "            #Incrementing the row counter\n",
    "            count += 1\n",
    "\n",
    "            #Save the images\n",
    "            with open(os.path.join(webScraper.NEWS_IMG_PATH, imageName),'wb') as file:\n",
    "                file.write(img_data)\n",
    "\n",
    "        #Append a list containing the URL, Article Title, Article Date of Publication, Categories and Image Names to the articleData list\n",
    "        articleData.append([url, title, date, categories, images[:-1]])\n",
    "        #Creating a dataframe using articleData and saving it in a CSV file named Article_Information.csv\n",
    "        pd.DataFrame(columns=['URL', 'Article Name', 'Date', 'Categories', 'Images'], data=articleData).to_csv(os.path.join(m.NEWS_PATH,'Article_Information.csv'), index=False)\n",
    "\n",
    "        #Go back to previous page\n",
    "        driver.back()\n",
    "\n",
    "    #Exception where element is not found\n",
    "    except selenium.common.exceptions.NoSuchElementException as e:\n",
    "        #Wait for the contents to load\n",
    "        sleep(1.5)\n",
    "\n",
    "        try: \n",
    "            #Go to the Next Page\n",
    "            driver.find_element(By.XPATH,f'//*[@id=\"observer\"]/main/div/div[2]/div/span[{str(2+pageIndex)}]').click()\n",
    "        except: \n",
    "            #Display Error Message and go back to Google Home Page\n",
    "            print(f\"ERROR: Reloading page {pageIndex+2}\")\n",
    "            driver.get(f\"https://google.com\")\n",
    "\n",
    "            #Wait for the contents to load\n",
    "            sleep(2)\n",
    "\n",
    "            #Increment Page Index\n",
    "            pageIndex += 1\n",
    "            #Load the website again, skipping the previous page\n",
    "            driver.get(f'https://timesofmalta.com/search?keywords=inset&author=0&tags=0&sort=date&order=desc&fields%5B0%5D=title&fields%5B1%5D=body&page={str(page_index+2)}') #Skip page\n",
    "\n",
    "            #Reset articleIndex\n",
    "            articleIndex = 0\n",
    "            \n",
    "            continue\n",
    "\n",
    "        #Increment pageIndex\n",
    "        pageIndex += 1\n",
    "        #Reset articleIndex\n",
    "        articleIndex = 0\n",
    "    \n",
    "    #Exception where element could not be clicked\n",
    "    except selenium.common.exceptions.ElementClickInterceptedException as e:\n",
    "        #Display Error Message\n",
    "        print('ERROR: Click Intercepted - Skipping')\n",
    "        #Increment articleIndex\n",
    "        articleIndex += 1\n",
    "\n",
    "        continue\n",
    "    \n",
    "    #Exception for other cases\n",
    "    except Exception as e:\n",
    "        #Display Error Message\n",
    "        print(str(e))\n",
    "\n",
    "        #Increment articleIndex\n",
    "        articleIndex += 1\n",
    "        #Increment pageIndex\n",
    "        pageIndex += 1\n",
    "        \n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = WebScraper(folder_name = '.\\\\times_of_malta')\n",
    "\n",
    "# # def signal_handler(sig, frame):\n",
    "# #     pd.DataFrame(columns=['Title','Image Name','Caption','Body'], data=data).to_csv(os.path.join(m.NEWS_PATH,'data.csv'), index=False)\n",
    "# #     sys.exit()\n",
    "\n",
    "# def get_img_ext(img_link):\n",
    "#     if img_link.endswith('.jpeg'):\n",
    "#         return '.jpeg'\n",
    "#     elif img_link.endswith('.jpg'):\n",
    "#         return '.jpg'\n",
    "#     elif img_link.endswith('.png'):\n",
    "#         return '.png'\n",
    "#     else: return \"\"\n",
    "\n",
    "# options = webdriver.ChromeOptions()\n",
    "# options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "\n",
    "# service = Service(executable_path=m.CHROME_DRIVER_PATH)\n",
    "\n",
    "# # Since chromedriver is in PATH we dont have specify it location otherwise webdriver.Chrome('path/chromedriver.exe')\n",
    "# driver = webdriver.Chrome(service = service, options = options)\n",
    "\n",
    "# # Opening required website to scrape content \n",
    "# driver.get('https://timesofmalta.com/search?keywords=inset&author=0&tags=0&sort=date&order=desc&fields%5B0%5D=title&fields%5B1%5D=body&page=1')\n",
    "\n",
    "\n",
    "# #Closing pop-ups\n",
    "# # print('Closing initial pop-ups: ',end='')\n",
    "# # driver.find_element(By.XPATH,'/html/body/div[4]/div[2]/div[1]/div[2]/div[2]/button[1]').click()\n",
    "# # print('[OK]')\n",
    "\n",
    "\n",
    "# data = []\n",
    "# count = 0\n",
    "# article_index = 0\n",
    "# page_index = 0\n",
    "\n",
    "# #Setup ^C Handler to save on signal.\n",
    "# try: \n",
    "#     # print(driver.find_element(By.CLASS_NAME,'fc-button fc-cta-consent fc-primary-button'))\n",
    "#     # print(\"FOUND BUT NOT CLICKED\")\n",
    "#     WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \n",
    "#                                                                 \"//div[@class='fc-consent-root']//button[@aria-label='Consent']//p[@class='fc-button-label' and text()='Consent']\"))).click()\n",
    "#     #driver.find_element(By.CLASS_NAME,\"fc-button fc-cta-consent fc-primary-button\").click()\n",
    "# except: \n",
    "#     pass\n",
    "# for i in range(10):\n",
    "\n",
    "#     # signal.signal(signal.SIGINT, signal_handler)\n",
    "#     try: \n",
    "#         #Remove donation message\n",
    "#         try: driver.find_element(By.XPATH,'//*[@id=\"eng-accept\"]').click()\n",
    "#         except: pass\n",
    "\n",
    "#         #Click on article\n",
    "#         driver.find_element(By.XPATH,f'//*[@id=\"listing-articles\"]/div[{str(2+article_index)}]/a').click()\n",
    "#         #Increment article index\n",
    "#         article_index += 1\n",
    "\n",
    "#         sleep(1)\n",
    "\n",
    "#         url = driver.current_url\n",
    "#         #Get Title\n",
    "#         title = driver.find_element(By.XPATH,'//*[@id=\"article-head\"]/div/h1').text\n",
    "\n",
    "#         print(f'Title: {title}')\n",
    "        \n",
    "#         #Get Thumbnail + Images\n",
    "#         img_links = [img for img in \\\n",
    "#                      driver.find_elements(By.XPATH,'//*[@id=\"observer\"]/main/article/div[2]/div/*/img') +\\\n",
    "#                      driver.find_elements(By.XPATH,'//*[@id=\"article-head\"]/div/picture/img')]\n",
    "        \n",
    "#         captions = \"\"\n",
    "#         images   = \"\"\n",
    "\n",
    "#         #Write images to disk\n",
    "#         for img_link in img_links:\n",
    "#             img_src   = img_link.get_attribute('src')        #Get source\n",
    "#             captions  += img_link.get_attribute('alt') + '☺' #Append current caption\n",
    "\n",
    "#             img_ext  = get_img_ext(img_src)                #Get image extension\n",
    "#             img_name = f'img{str(count).zfill(5)}'+img_ext #Get image name\n",
    "#             images  += img_name + ','                      #Append image name to list\n",
    "#             img_data = requests.get(img_src).content       #Download image\n",
    "\n",
    "#             count += 1\n",
    "\n",
    "#             #Write current image to disk\n",
    "#             with open(os.path.join(m.NEWS_IMG_PATH,img_name),'wb') as f:\n",
    "#                 f.write(img_data)\n",
    "\n",
    "#         #Get Body\n",
    "#         text_content = driver.find_element(By.XPATH,'//*[@id=\"observer\"]/main/article/div[2]/div')                                                     \n",
    "#         body = \" \".join(p.text for p in text_content.find_elements(By.TAG_NAME,'p'))\n",
    "\n",
    "#         #Add row\n",
    "#         data.append([url, images[:-1], captions[:-1]])\n",
    "\n",
    "#         #Save to csv\n",
    "#         # if i%50 == 0:\n",
    "#         print('Saving...\\n')\n",
    "#         for image in images:\n",
    "#             pd.DataFrame(columns=['Image Name','Caption', 'Inset'], data=data).to_csv(os.path.join(m.NEWS_PATH,'contents.csv'), index=False)\n",
    "\n",
    "#     #Go back\n",
    "#         print('Back to home page\\n')\n",
    "#         driver.back()\n",
    "\n",
    "#     except selenium.common.exceptions.NoSuchElementException as e:\n",
    "#         #Switch to next page\n",
    "#         sleep(1.5)\n",
    "\n",
    "#         try: driver.find_element(By.XPATH,f'//*[@id=\"observer\"]/main/div/div[2]/div/span[{str(2+page_index)}]').click()\n",
    "#         except: \n",
    "#             print(f'Error: Reloading page {page_index+2}')\n",
    "#             driver.get(f'https://google.com')\n",
    "#             sleep(2)\n",
    "#             page_index += 1\n",
    "#             driver.get(f'https://timesofmalta.com/search?keywords=inset&author=0&tags=0&sort=date&order=desc&fields%5B0%5D=title&fields%5B1%5D=body&page={str(page_index+2)}') #Skip page\n",
    "\n",
    "#             try: driver.find_element(By.XPATH,'//*[@id=\"qc-cmp2-ui\"]/div[2]/div/button[2]').click() #Close pop-up\n",
    "#             except: pass\n",
    "\n",
    "#             article_index = 0 #Invoke next-page switch\n",
    "            \n",
    "#             continue #Go back to beggining\n",
    "\n",
    "        \n",
    "\n",
    "#         page_index   += 1\n",
    "#         article_index = 0\n",
    "#         print(f'Next Page! -> {page_index+1}')\n",
    "    \n",
    "#     except selenium.common.exceptions.ElementClickInterceptedException as e:\n",
    "#         print('Click Intercepted - Skipping')\n",
    "#         article_index += 1\n",
    "#         continue\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(str(e))\n",
    "#         article_index += 1\n",
    "#         page_index += 1\n",
    "#         continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
